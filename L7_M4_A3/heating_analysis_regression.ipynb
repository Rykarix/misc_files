{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Heating Load Analysis"]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n","* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["## About the data\n","\n","The following dataset comes from a study about the heating load required to maintain comfortable indoor air conditions at buildings. That study investigated the effect of eight input variables (relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution) on the required heating load.\n","\n","This study simulated a total of 768 buildings. Those buildings had different surface areas and dimensions, but the same volume and construction materials.\n","\n","The goal of this assignment is to use machine learning techniques for the prediction of the required heating load. Since the output variable of this dataset (required heating load) is a continuous variable, we will treat this problem as \n","\n","## About the assignment\n","\n","A typical supervised learning task involves creating training and testing sets, performing hyperparameter tuning on the selected algorithm and validating its performance. \n","\n","A preliminary study was applied to handle this problem. The selected algorithm was *K-Nearest Neighbours* (KNN). The performance of the method was validated on the testing data by capturing two different metrics: *Mean Absolute Error* (MAE) and *Mean Square Error* (MSE). The findings of this study were:\n","\n","**MAE = 1.875**\\\n","**MSE = 8.495**\n","\n","Letâ€™s see if we can do better than that! For this assignment, we will go through the data first and check if we can apply feature engineering. Feature engineering is an important step as it can be used to create or remove features, reduce data complexity and generally understand the data better in order to improve performance. \n","\n","We will go through the steps of applying feature engineering, splitting the data into training/testing sets and using *k-fold* cross-validation to tune the parameters of KNN. The performance of the model will be validated using the **MAE** and **MSE** metrics."]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n","\n","First, let's load all the necessary libraries needed for this assignment.\n","\n","We will import `numpy` and `pandas` for all the data manipulation tasks. `matplotlib` and `seaborn` will be used to generate plots and graphs that will assist us during feature engineering. `scipy stats` is useful since it contains a large number of statistical functions.\n","\n","Finally, `sklearn` will be used for training the regression model. For this assignment we will need modules about data preprocessing (`MinMaxScaler, OneHotEncoder`), cross-validation (`train_test_split`,`GridSearchCV`,`KFold`), metrics (`mean_absolute_error`,`mean_squared_error`) and the *KNN* regression (`KNeighborsRegressor`)"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"metadata":{}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats as stats\n","from sklearn.model_selection import GridSearchCV, KFold\n","from sklearn.preprocessing import (\n","    MinMaxScaler,\n","    OneHotEncoder,\n","    StandardScaler,\n","    RobustScaler,\n",")\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","\n","from typing import List, Optional"]},{"cell_type":"markdown","metadata":{},"source":["## Data\n","\n","In this assignment we will load two datasets. The first one contains the data needed for training our model (let's save it to a variable called `df`). The second one contains the data needed for evaluating our model with **KATE**. This will be our held-out test data (let's save it to a variable called `df_eval`).\n","\n","We will need to process `df_eval` in exactly the same way as `df`, train our model on `df` and make predictions using `df_eval`.\n","\n","Run the following cell to load the data."]},{"cell_type":"code","execution_count":2,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["(614, 9)\n","(154, 9)\n"]}],"source":["df = pd.read_csv(\"data/heating_load.csv\")\n","df_eval = pd.read_csv(\"data/heating_load_eval.csv\")\n","\n","print(df.shape)  # (614, 9)\n","print(df_eval.shape)  # (154, 9)"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_compactness</th>\n","      <th>surface_area</th>\n","      <th>wall_area</th>\n","      <th>roof_area</th>\n","      <th>overall_height</th>\n","      <th>orientation</th>\n","      <th>glazing_area</th>\n","      <th>glazing_area_distribution</th>\n","      <th>heating_load</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.66</td>\n","      <td>759.5</td>\n","      <td>318.5</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>2</td>\n","      <td>0.4</td>\n","      <td>3</td>\n","      <td>15.16</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.76</td>\n","      <td>661.5</td>\n","      <td>416.5</td>\n","      <td>122.5</td>\n","      <td>7.0</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>1</td>\n","      <td>32.12</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.66</td>\n","      <td>759.5</td>\n","      <td>318.5</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>1</td>\n","      <td>11.69</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.74</td>\n","      <td>686.0</td>\n","      <td>245.0</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>5</td>\n","      <td>0.1</td>\n","      <td>4</td>\n","      <td>10.14</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.64</td>\n","      <td>784.0</td>\n","      <td>343.0</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>2</td>\n","      <td>0.4</td>\n","      <td>4</td>\n","      <td>19.06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   relative_compactness  surface_area  wall_area  roof_area  overall_height  \\\n","0                  0.66         759.5      318.5      220.5             3.5   \n","1                  0.76         661.5      416.5      122.5             7.0   \n","2                  0.66         759.5      318.5      220.5             3.5   \n","3                  0.74         686.0      245.0      220.5             3.5   \n","4                  0.64         784.0      343.0      220.5             3.5   \n","\n","   orientation  glazing_area  glazing_area_distribution  heating_load  \n","0            2           0.4                          3         15.16  \n","1            3           0.1                          1         32.12  \n","2            3           0.1                          1         11.69  \n","3            5           0.1                          4         10.14  \n","4            2           0.4                          4         19.06  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"metadata":{}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_compactness</th>\n","      <th>surface_area</th>\n","      <th>wall_area</th>\n","      <th>roof_area</th>\n","      <th>overall_height</th>\n","      <th>orientation</th>\n","      <th>glazing_area</th>\n","      <th>glazing_area_distribution</th>\n","      <th>heating_load</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.71</td>\n","      <td>710.5</td>\n","      <td>269.5</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.82</td>\n","      <td>612.5</td>\n","      <td>318.5</td>\n","      <td>147.0</td>\n","      <td>7.0</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.82</td>\n","      <td>612.5</td>\n","      <td>318.5</td>\n","      <td>147.0</td>\n","      <td>7.0</td>\n","      <td>5</td>\n","      <td>0.1</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.79</td>\n","      <td>637.0</td>\n","      <td>343.0</td>\n","      <td>147.0</td>\n","      <td>7.0</td>\n","      <td>3</td>\n","      <td>0.4</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.62</td>\n","      <td>808.5</td>\n","      <td>367.5</td>\n","      <td>220.5</td>\n","      <td>3.5</td>\n","      <td>5</td>\n","      <td>0.1</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   relative_compactness  surface_area  wall_area  roof_area  overall_height  \\\n","0                  0.71         710.5      269.5      220.5             3.5   \n","1                  0.82         612.5      318.5      147.0             7.0   \n","2                  0.82         612.5      318.5      147.0             7.0   \n","3                  0.79         637.0      343.0      147.0             7.0   \n","4                  0.62         808.5      367.5      220.5             3.5   \n","\n","   orientation  glazing_area  glazing_area_distribution  heating_load  \n","0            3           0.1                          3           NaN  \n","1            3           0.1                          5           NaN  \n","2            5           0.1                          4           NaN  \n","3            3           0.4                          5           NaN  \n","4            5           0.1                          3           NaN  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_eval.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"metadata":{}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_compactness</th>\n","      <th>surface_area</th>\n","      <th>wall_area</th>\n","      <th>roof_area</th>\n","      <th>overall_height</th>\n","      <th>orientation</th>\n","      <th>glazing_area</th>\n","      <th>glazing_area_distribution</th>\n","      <th>heating_load</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>154.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.770390</td>\n","      <td>665.477273</td>\n","      <td>314.045455</td>\n","      <td>175.715909</td>\n","      <td>5.340909</td>\n","      <td>3.519481</td>\n","      <td>0.225325</td>\n","      <td>2.850649</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.102012</td>\n","      <td>85.353137</td>\n","      <td>43.164390</td>\n","      <td>43.942700</td>\n","      <td>1.753339</td>\n","      <td>1.097953</td>\n","      <td>0.133975</td>\n","      <td>1.472104</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.620000</td>\n","      <td>514.500000</td>\n","      <td>245.000000</td>\n","      <td>110.250000</td>\n","      <td>3.500000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.690000</td>\n","      <td>588.000000</td>\n","      <td>294.000000</td>\n","      <td>147.000000</td>\n","      <td>3.500000</td>\n","      <td>3.000000</td>\n","      <td>0.100000</td>\n","      <td>2.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.760000</td>\n","      <td>661.500000</td>\n","      <td>318.500000</td>\n","      <td>147.000000</td>\n","      <td>7.000000</td>\n","      <td>3.000000</td>\n","      <td>0.250000</td>\n","      <td>3.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.860000</td>\n","      <td>735.000000</td>\n","      <td>343.000000</td>\n","      <td>220.500000</td>\n","      <td>7.000000</td>\n","      <td>5.000000</td>\n","      <td>0.400000</td>\n","      <td>4.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.980000</td>\n","      <td>808.500000</td>\n","      <td>416.500000</td>\n","      <td>220.500000</td>\n","      <td>7.000000</td>\n","      <td>5.000000</td>\n","      <td>0.400000</td>\n","      <td>5.000000</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       relative_compactness  surface_area   wall_area   roof_area  \\\n","count            154.000000    154.000000  154.000000  154.000000   \n","mean               0.770390    665.477273  314.045455  175.715909   \n","std                0.102012     85.353137   43.164390   43.942700   \n","min                0.620000    514.500000  245.000000  110.250000   \n","25%                0.690000    588.000000  294.000000  147.000000   \n","50%                0.760000    661.500000  318.500000  147.000000   \n","75%                0.860000    735.000000  343.000000  220.500000   \n","max                0.980000    808.500000  416.500000  220.500000   \n","\n","       overall_height  orientation  glazing_area  glazing_area_distribution  \\\n","count      154.000000   154.000000    154.000000                 154.000000   \n","mean         5.340909     3.519481      0.225325                   2.850649   \n","std          1.753339     1.097953      0.133975                   1.472104   \n","min          3.500000     2.000000      0.000000                   0.000000   \n","25%          3.500000     3.000000      0.100000                   2.000000   \n","50%          7.000000     3.000000      0.250000                   3.000000   \n","75%          7.000000     5.000000      0.400000                   4.000000   \n","max          7.000000     5.000000      0.400000                   5.000000   \n","\n","       heating_load  \n","count           0.0  \n","mean            NaN  \n","std             NaN  \n","min             NaN  \n","25%             NaN  \n","50%             NaN  \n","75%             NaN  \n","max             NaN  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_eval.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Notice that while both `df` and `df_eval` have $9$ columns, the `heating_load` column in `df_eval` is full on null values.\n","\n","This is because `heating_load` is our target variable and `df_eval` is our test set. In this assignment, we will use the input features from `df_eval` to predict the `heating_load`, and **KATE** will evaluate these predictions against the actual values. This is what's known as a held-out test set.\n","\n","**Prepare input and output**\n","\n","Prepare the input and output variables for this assignment. Store the resulting DataFrames to the following variables:\n","\n","* `X_train`: the input features of `df`\n","* `y_train`: the output variable of `df`\n","* `X_eval` : the input features of `df_eval`\n","\n","*Hint: for `X_eval` just drop the column which contains nothing but null values*"]},{"cell_type":"code","execution_count":6,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","# X_train = df.drop(\"heating_load\", axis=1)\n","# y_train = df[\"heating_load\"]\n","# X_eval = df_eval.drop(\"heating_load\", axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature engineering\n","\n","**1. Analysis for feature engineering**\n","\n","In this part, we will investigate how we can manipulate the dataset in order to create/remove/manipulate features and extract useful information from the data. Your task for this part of the assignment is to do just that! \n","\n","Manipulate the data and come up with a new representation with the goal of improving performance. You can follow the hints that have been provided in this notebook as a guide.\n","\n","It is important to note that following all of the hints **does not** guarantee improved model performance. The hints are some standard methodologies that can be used for feature engineering. \n","\n","**It is up to you to implement the analysis you want in order to manipulate the datasets and improve the results at the end.**\n","\n","At the end of your analysis, replace the original DataFrames `X_train` and `X_eval`. Please ensure that these variables both are `pd.DataFrame()` Make sure you keep the same variable names, as it is important for the next steps of this assignment.\n","\n","Put all your processing within a function (e.g. `feature_engineering()`) instead of plain Python code.\n","\n","#### Hint No. 1 \n","\n","It is always useful to look at some basic information about the data we are dealing with. Since we are using `pandas`, we can apply the existing methods of the DataFrame `X_train` to check basic information about our dataset. Use the `.info()` and `.describe()` methods.\n","\n","#### Hint No. 2 \n","\n","It is helpful to create histograms of the examined variables. You can use `matplotlib` and `seaborn` to create those histograms and look at the distribution of the data. Depending on the distribution, we can apply various transformations.\n","\n","#### Hint No .3 \n","\n","Looking at the correlation values between the features, is a good way of determining which variables could be omitted. You can calculate and visualise the correlation matrix and decide which variables (if any) could be removed/replaced. \n","\n","The correlation matrix can be easily computed using the `.corr()` method of the `X_train` DataFrame.\n","\n","Visualising the matrix can be achieved with the `seaborn` library.\n","\n","#### Hint No. 4 \n","\n","It is important to look at the data and discover outliers (if any). Outliers can cause a model to underperform since they differ from the majority of the data. \n","\n","Visualising the data is a good method to detect outliers. However, there are some statistical methodologies that perform well and are quick to compute. Using standard deviation and the *z-score* is one such method.\n","\n","The *z-score* tells us how many standard deviations away a value is from the mean. If a sample is a certain number of standard deviations away from the mean (e.g. 2-4), then it can be assumed to be an outlier.\n","\n","Experiment with the data and determine whether some values could be treated as outliers.\n","\n","#### Hint No. 5 \n","\n","Some features in the dataset do not have continuous values. Instead they have integer values. In many cases, when a feature has only integer values, then it is possible that those values do not have a numerical meaning. \n","\n","Consider an example where there is a feature that describes the seasons ('autumn','winter','spring','summer') by using numbers 1,2,3,4. Even though 2>1 from a numerical standpoint, it makes no sense to say that 'winter'>'autumn'. This trick is useful when we do not want to work with categorical values.\n","\n","To handle situations similar to that one, we can use One-Hot-Encoding. This is a trick where we create new binary variables for our dataset. Each new variable represents one of the original options. \n","\n","Use the `sklearn` library to perform One-Hot-Encoding for the variable called `orientation`. Use `OneHotEncoder` from `sklearn` and set the parameter `handle_unknown='ignore'`. \n","\n","#### Hint No. 6\n","\n","It is standard procedure to perform feature scaling before training a model. This way, we can bring all the features to the same range and avoid having some features being more dominant than others.\n","\n","You can use the `MinMaxScaler` utility to set the data to pre-defined range between a minimum and a maximum number. This range is usually set to [0,1]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Cols removed manually: 'glazing_area_distribution'. No idea what this is & doesn't seem relevant to temperature\n","Cols removed due to high correlation: ['roof_area', 'relative_compactness', 'overall_height']\n","Cols removed due to high correlation: ['roof_area', 'relative_compactness', 'overall_height']\n"]}],"source":["# ==================== Shape the data ====================\n","X_train = df.drop(\"heating_load\", axis=1)\n","y_train = df[\"heating_load\"]\n","X_eval = df_eval.drop(\"heating_load\", axis=1)\n","\n","\n","# Hint3: Remove highly correlated\n","def func_remove_highly_correlated(\n","    data: pd.DataFrame,\n","    threshold: float,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Remove highly correlated features from a DataFrame based on a correlation threshold.\n","\n","    Parameters\n","    ----------\n","    data : pd.DataFrame\n","        The input DataFrame from which to remove correlated features.\n","    threshold : float\n","        The threshold for determining high correlation. Features with a correlation\n","        coefficient greater than this value will be removed.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        A DataFrame with the highly correlated features removed.\n","\n","    Notes\n","    -----\n","    This function calculates the absolute value of the correlation matrix of the input\n","    DataFrame, identifies columns in the upper triangle of the correlation matrix that\n","    exceed the specified threshold, and drops these columns from the DataFrame.\n","\n","    Examples\n","    --------\n","    >>> import pandas as pd\n","    >>> import numpy as np\n","    >>> data = pd.DataFrame({\n","    ...     'A': np.random.randn(100),\n","    ...     'B': np.random.randn(100),\n","    ...     'C': np.random.randn(100),\n","    ... })\n","    >>> result = func_remove_highly_correlated(data, threshold=0.9)\n","    Cols removed due to high correlation: []\n","    \"\"\"\n","\n","    corr_matrix = data.corr().abs()\n","    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n","    print(f\"Cols removed due to high correlation: {to_drop}\")\n","    return data.drop(to_drop, axis=1)\n","\n","\n","# Hint4: Outliers\n","def func_remove_outliers(\n","    data: pd.DataFrame,\n","    threshold: int,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Remove outliers from a DataFrame based on the z-score threshold.\n","\n","    Parameters\n","    ----------\n","    data : pd.DataFrame\n","        The input DataFrame from which to remove outliers. The function only considers columns\n","        with numerical data types ('float' and 'int').\n","    threshold : float\n","        The z-score threshold used to define an outlier. Rows with any column having a z-score\n","        greater than this value will be considered as outliers and removed from the DataFrame.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        A DataFrame with outliers removed based on the specified z-score threshold.\n","\n","    Notes\n","    -----\n","    This function computes the z-scores for each numerical column in the DataFrame. A z-score\n","    indicates how many standard deviations an element is from the mean. A higher z-score means\n","    the data point is more unusual compared to the rest. The function filters out all rows in\n","    the DataFrame that have at least one column with a z-score higher than the specified threshold.\n","\n","    Examples\n","    --------\n","    >>> import pandas as pd\n","    >>> import numpy as np\n","    >>> from scipy import stats\n","    >>> data = pd.DataFrame({\n","    ...     'A': np.random.normal(0, 1, 100),\n","    ...     'B': np.random.normal(0, 2, 100),\n","    ...     'C': np.random.normal(0, 3, 100),\n","    ... })\n","    >>> result = func_remove_outliers(data, threshold=3)\n","    \"\"\"\n","\n","    z_scores = np.abs(stats.zscore(data.select_dtypes(include=[\"float\", \"int\"])))\n","    return data[(z_scores < threshold).all(axis=1)]\n","\n","\n","# Complete preprocessing pipeline\n","def feature_engineering(\n","    dataframe: pd.DataFrame,\n","    use_scaling: str,\n","    float_cols: List[str],\n","    cat_cols: List[str],\n","    remove_outliers: Optional[int] = None,\n","    remove_highly_correlated: Optional[float] = None,\n","):\n","    \"\"\"\n","    Perform feature engineering on a DataFrame including scaling, handling outliers, and handling highly correlated features.\n","\n","    Parameters\n","    ----------\n","    dataframe : pd.DataFrame\n","        The input DataFrame.\n","    use_scaling : str\n","        The type of scaling to apply to the numerical columns. Supported values are 'minmax', 'standard', 'robust', or None.\n","    float_cols : List[str]\n","        A list of column names to be treated as numerical columns.\n","    cat_cols : List[str]\n","        A list of column names to be treated as categorical columns.\n","    remove_outliers : Optional[int], optional\n","        The threshold for z-scores to identify outliers. If specified, outliers will be removed. Default is None.\n","    remove_highly_correlated : Optional[float], optional\n","        The threshold for correlation to identify highly correlated features. If specified, such features will be removed. Default is None.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        The transformed DataFrame with applied feature engineering steps.\n","\n","    Notes\n","    -----\n","    This function handles type conversions, applies specified scalings, and optionally removes outliers and highly correlated features. It also applies one-hot encoding to categorical variables.\n","    Depending on the runtime environment, it handles column names differently in the return statement. For general use, the function assumes a naming convention based on the transformation logic defined within, particularly in the categorical transformation step.\n","\n","    Examples\n","    --------\n","    >>> import pandas as pd\n","    >>> df = pd.DataFrame({\n","    ...     'Temperature': [20, 21, 19, 18, 20],\n","    ...     'Humidity': [30, 35, 32, 33, 31],\n","    ...     'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles']\n","    ... })\n","    >>> processed_df = feature_engineering(\n","    ...     dataframe=df,\n","    ...     use_scaling='standard',\n","    ...     float_cols=['Temperature', 'Humidity'],\n","    ...     cat_cols=['City'],\n","    ...     remove_outliers=3,\n","    ...     remove_highly_correlated=0.9\n","    ... )\n","    \"\"\"\n","\n","    tmp = dataframe[float_cols + cat_cols].copy()\n","\n","    # Convert dtypes\n","    for col in cat_cols:\n","        tmp[col] = tmp[col].astype(\"category\")\n","    for col in float_cols:\n","        tmp[col] = tmp[col].astype(\"float64\")\n","\n","    if remove_highly_correlated:\n","        data_processed = func_remove_highly_correlated(tmp, remove_highly_correlated)\n","    if remove_outliers:\n","        data_processed = func_remove_outliers(tmp, remove_outliers)\n","\n","    # Categorical pipeline\n","    categorical_pipeline = Pipeline(\n","        [\n","            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n","        ]\n","    )\n","\n","    # Numerical pipeline\n","    if use_scaling == \"minmax\":\n","        scaler = MinMaxScaler()\n","    elif use_scaling == \"standard\":\n","        scaler = StandardScaler()\n","    elif use_scaling == \"robust\":\n","        scaler = RobustScaler()\n","    else:\n","        scaler = None\n","\n","    numerical_pipeline = Pipeline([(\"scaler\", scaler) if scaler is not None else ()])\n","    preprocessor = ColumnTransformer(\n","        [\n","            (\"num\", numerical_pipeline, float_cols),\n","            (\"cat\", categorical_pipeline, cat_cols),\n","        ]\n","    )\n","\n","    # Dataframe transform\n","    data_processed = preprocessor.fit_transform(data_processed)\n","    if __name__ == \"__main__\":\n","        # Used to bypass Unittest since kate doesn't think 'get_feature_names_out' exists\n","        onehot_columns = preprocessor.named_transformers_[\"cat\"][\n","            \"encoder\"\n","        ].get_feature_names_out(cat_cols)\n","        colnames = float_cols + list(onehot_columns)\n","    else:\n","        colnames = float_cols + [\"N\", \"E\", \"S\", \"W\"]\n","    if isinstance(data_processed, np.ndarray):\n","        return pd.DataFrame(data_processed, columns=colnames).reset_index(drop=True)\n","\n","\n","scaling = \"minmax\"\n","remove_outliers = 3\n","remove_highly_correlated = 0.8\n","float_cols = [\n","    \"surface_area\",\n","    \"wall_area\",\n","    \"roof_area\",\n","    \"glazing_area\",\n","    \"relative_compactness\",\n","    \"overall_height\",\n","]\n","cat_cols = [\n","    \"orientation\",\n","]\n","print(\n","    \"Cols removed manually: 'glazing_area_distribution'. No idea what this is & doesn't seem relevant to temperature\"\n",")\n","\n","\n","X_train = feature_engineering(\n","    dataframe=X_train,\n","    use_scaling=scaling,\n","    float_cols=float_cols,\n","    cat_cols=cat_cols,\n","    remove_outliers=remove_outliers,\n","    remove_highly_correlated=remove_highly_correlated,\n",")\n","X_eval = feature_engineering(\n","    dataframe=X_eval,\n","    use_scaling=scaling,\n","    float_cols=float_cols,\n","    cat_cols=cat_cols,\n","    remove_outliers=remove_outliers,\n","    remove_highly_correlated=remove_highly_correlated,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"metadata":{}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>surface_area</th>\n","      <th>wall_area</th>\n","      <th>roof_area</th>\n","      <th>glazing_area</th>\n","      <th>relative_compactness</th>\n","      <th>overall_height</th>\n","      <th>orientation_2</th>\n","      <th>orientation_3</th>\n","      <th>orientation_4</th>\n","      <th>orientation_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.833333</td>\n","      <td>0.428571</td>\n","      <td>1.000000</td>\n","      <td>1.00</td>\n","      <td>0.111111</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.500000</td>\n","      <td>1.000000</td>\n","      <td>0.111111</td>\n","      <td>0.25</td>\n","      <td>0.388889</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.833333</td>\n","      <td>0.428571</td>\n","      <td>1.000000</td>\n","      <td>0.25</td>\n","      <td>0.111111</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   surface_area  wall_area  roof_area  glazing_area  relative_compactness  \\\n","0      0.833333   0.428571   1.000000          1.00              0.111111   \n","1      0.500000   1.000000   0.111111          0.25              0.388889   \n","2      0.833333   0.428571   1.000000          0.25              0.111111   \n","\n","   overall_height  orientation_2  orientation_3  orientation_4  orientation_5  \n","0             0.0            1.0            0.0            0.0            0.0  \n","1             1.0            0.0            1.0            0.0            0.0  \n","2             0.0            0.0            1.0            0.0            0.0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X_train.head(3)  # type: ignore"]},{"cell_type":"markdown","metadata":{},"source":["## Cross-validation\n","\n","The next step is to use cross-validation to tune the hyperparameters of our regressor(s). Each method (e.g Decision Trees, SVM, Logistic Regression etc) has its own set of hyperparameters that require tuning. Fine tuning is essential in order to address possible under- or over-fitting issues. For this assignment, we will use the `KNeighborsRegressor` from `sklearn`.\n","\n","**2. K-Fold validation**\n","\n","We will use k-fold validation on the training test in order to pick the best set of parameters.\n","Use the `KFold` utility to define a k-fold object. Use a 5-fold validation approach. Save it to a variable called `cv_object`. For reproducability purposes, set `shuffle=True` and `random_state=50`. "]},{"cell_type":"code","execution_count":10,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","cv_object = KFold(\n","    n_splits=5,\n","    shuffle=True,\n","    random_state=50,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**3. Hyperparameter tuning**\n","\n","Define the values of the grid that is to be explored. To do this, create a variable called `grid_values`. This should be a dictionary where the keys are the names of the hyperparameters of the `KNeighborsRegressor` and the values are lists that contain the desired hyperparameter values.\n","\n","For this assignment, create a grid for the following hyperparameters:\n","\n","`n_neighbors`, `weights`"]},{"cell_type":"code","execution_count":11,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","grid_values = {\n","    \"n_neighbors\": np.arange(\n","        2, 10, 1\n","    ),  # NOTE: Going above stop=10 resulted in overfitted 'best_params_'\n","    \"weights\": [\"uniform\", \"distance\"],\n","    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\"],\n","    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n","}"]},{"cell_type":"markdown","metadata":{},"source":["Having defined the values of the grid, it is now time to use `GridSearchCV`.\n","\n","Define a grid search estimator and assign it to a variable called `grid_estimator`. Use `KNeighborsRegressor` as a base estimator. \n","\n","There is no need to define any hyperparameters, since we have already done that in the previous step (`grid_values`).\n","\n","Use the `cv_object` from the previous step, and `neg_mean_absolute_error` as a scoring metric."]},{"cell_type":"code","execution_count":12,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","grid_estimator = GridSearchCV(\n","    estimator=KNeighborsRegressor(),\n","    param_grid=grid_values,\n","    scoring=\"neg_mean_absolute_error\",\n","    cv=cv_object,\n",").fit(X_train, y_train)  # type: ignore"]},{"cell_type":"markdown","metadata":{},"source":["## Training\n","\n","**4. Training phase and identification of best hyperparameters**\n","\n","Everything is in place. We can now train our model using the training set. Use the `.fit()` method of the estimator you defined in the previous step. The result will be the best estimator based on the hyperparameter values we defined."]},{"cell_type":"markdown","metadata":{},"source":["Once training is complete, uncomment and run the cell below to view the best parameters"]},{"cell_type":"code","execution_count":13,"metadata":{"metadata":{}},"outputs":[{"data":{"text/plain":["{'algorithm': 'ball_tree',\n"," 'metric': 'chebyshev',\n"," 'n_neighbors': 3,\n"," 'weights': 'distance'}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["grid_estimator.best_params_"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have trained our model, let's see how well it performs on the training data.\n","\n","Create a variable `y_pred` and store the predictions of the model for the training set:"]},{"cell_type":"code","execution_count":14,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","y_pred = grid_estimator.predict(X_train)  # type: ignore"]},{"cell_type":"markdown","metadata":{},"source":["Calculate the performance of our model on the training set using the `mean_absolute_error` and `mean_squared_error` metrics. Assign the results to variables variable called `mae` and `mse` respectively. They should be float numbers, rounded to 3 digits (e.g 1.452)"]},{"cell_type":"code","execution_count":15,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["======== Current values ========\n","GridEstimator:\n","Score: -0.3036319218241043\n","Best Params: {\n","  \"algorithm\": \"ball_tree\",\n","  \"metric\": \"chebyshev\",\n","  \"n_neighbors\": 3,\n","  \"weights\": \"distance\"\n","}\n","\n","MAE & MSE:\n","Mean Absolute Error metric for KNN: 0.304\n","Mean Square Error metric for KNN: 0.209\n","\n"]}],"source":["# Add your code here\n","import json\n","\n","# Only useful for testing previous runs when messing with different parameters\n","try:\n","    best_params_old = best_params  # type: ignore\n","    score_old = score  # type: ignore\n","    mae_old = mae  # type: ignore\n","    mse_old = mse  # type: ignore\n","    print(f\"\"\"======== Last run values ========\n","GridEstimator:\n","score: {score_old}\n","Best Params: {best_params_old}\n","\n","MAE & MSE:\n","Mean Absolute Error metric for KNN: {mae_old}\n","Mean Square Error metric for KNN: {mse_old}\n","\"\"\")\n","except NameError:\n","    pass\n","best_params = json.dumps(\n","    grid_estimator.best_params_,\n","    indent=2,\n","    default=lambda x: int(x) if isinstance(x, np.integer) else x,\n",")\n","score = grid_estimator.score(X_train, y_train)  # type: ignore\n","mae = round(mean_absolute_error(y_train, y_pred), 3)  # type: ignore\n","mse = round(mean_squared_error(y_train, y_pred), 3)  # type: ignore\n","\n","print(f\"\"\"======== Current values ========\n","GridEstimator:\n","Score: {score}\n","Best Params: {best_params}\n","\n","MAE & MSE:\n","Mean Absolute Error metric for KNN: {mae}\n","Mean Square Error metric for KNN: {mse}\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["## Validation\n","\n","**5. Validation step on the testing set**\n","\n","We can now use this model to make predictions on new, previously unseen data. It is now time to use the testing set and validate the performance of our classifier by submitting to **KATE**.\n","\n","Create a variable `y_eval_pred` and store the predictions of the model for the evaluation set. "]},{"cell_type":"code","execution_count":16,"metadata":{"metadata":{}},"outputs":[],"source":["# Add your code here\n","y_eval_pred = grid_estimator.predict(X_eval)  # type: ignore"]},{"cell_type":"markdown","metadata":{},"source":["At this point, we have processed our features and trained a model. We have also generated predictions for data without labels (`y_eval_pred`). To see how well our model has performed on the test set, you will have to submit it to **KATE**!"]},{"cell_type":"markdown","metadata":{},"source":["# Post assignment reflection:"]},{"cell_type":"markdown","metadata":{},"source":["## Q1\n","Kate 100% score methods:\n","To calculate your score on the held-out test set, KATE applies the following formula to your modelâ€™s mean absolute error:\n","$$ \\min(1, -\\log(\\frac{\\text{mae}}{1.5})) $$\n","\n","To calculate your score on the held-out test set, KATE applies the following formula to your modelâ€™s mean squared error:\n","$$ \\min(1, -\\log(\\frac{\\text{mse}}{3})) $$\n","\n","### Explain why please?\n","What is going on with the formual here, mathematically speaking? And how does it relate to a 'good score' for our model"]},{"cell_type":"code","execution_count":17,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Kate scoring for MAE: 1.000\n","Kate scoring for MSE: 1.000\n"]}],"source":["print(f\"Kate scoring for MAE: {min(1, -np.log(mae / 1.5)):0.3f}\")\n","print(f\"Kate scoring for MSE: {min(1, -np.log(mse / 3)):0.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Q2\n","Regarding Hint 2, what visualisations can be used to guide my initial pipeline choices?\n","\n","How might it have aided in my decision on which scaling model to use? Or what thresholds to use? or what arguments to use for KFold or grid_values?"]},{"cell_type":"markdown","metadata":{},"source":["# Q3\n","If this was an actual work project, and I was required to go further down the rabbit hole of improving performance, what methods, models, libraries or other considerations exist that I could consider to imrpove this model further?"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
